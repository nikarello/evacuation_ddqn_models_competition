# algorithms/base_trainer.py
import abc, time, torch

import csv, pathlib, pandas as pd, matplotlib.pyplot as plt
import math, numpy as np

import imageio, os
from PIL import Image
import numpy as np
from datetime import datetime

from environment import stack_envs
from core import batched_update_fire_exit, batched_update_agents, \
                batched_get_views, batched_step

class BaseTrainer(abc.ABC):
    def __init__(self, envs, cfg):
        self.envs=envs; self.cfg=cfg
        self.device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.online = None
        self.target = None
        # ðŸ”§ Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
        self.in_ch      = 8 # ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ°Ð½Ð°Ð»Ð¾Ð² Ð¾Ð±Ð·Ð¾Ñ€Ð°
        self.view       = cfg["VIEW_SIZE"]
        self.gamma      = cfg["GAMMA"]
        self.batch_size = cfg["BATCH_SIZE"]
        self.memory_size= cfg.get("MEMORY_SIZE", 100_000)
        self.eps_start  = cfg.get("EPSILON_START", 1.0)
        self.eps_min    = cfg.get("EPSILON_MIN", 0.1)
        self.eps_decay  = cfg.get("EPSILON_DECAY", 0.995)
        self.n_envs     = cfg["NUM_ENVS"]
        self.max_steps  = cfg["MAX_STEPS_PER_EPISODE"]
        self.target_update_freq = cfg.get("TARGET_UPDATE_FREQ", 5)
        self.grid_size  = cfg["GRID_SIZE"]
        self.agent_specs= cfg["AGENT_SPECS"]

        self.lr         = cfg["LEARNING_RATE"]
        # ðŸ” Learning Rate decay
        self.lr_min     = cfg.get("LEARNING_RATE_MIN", 1e-5)
        self.lr_decay   = (self.lr_min / self.lr) ** (1.0 / cfg["NUM_EPISODES"])

    # ---- Ð°Ð±ÑÑ‚Ñ€Ð°ÐºÑ‚Ð½Ñ‹Ðµ ----
    @abc.abstractmethod
    def build_model(self): ...
    @abc.abstractmethod
    def select_actions(self, views, mask): ...
    @abc.abstractmethod
    def learn_step(self): ...
    @abc.abstractmethod
    def after_episode(self, ep): ...
    # ---- Ð¾Ð±Ñ‰Ð¸Ð¹ Ñ†Ð¸ÐºÐ» ----

    
    def train(self):

        B = len(self.envs)
        for e in self.envs:
            e.reset()

        self._init_metrics()
        

        for ep in range(self.cfg["NUM_EPISODES"]):
            start = time.time()
            pos, alive, know, hp, sz, sp, fire, exits, leader_pos, leader_alive = stack_envs(self.envs)

            pos        = pos.to(self.device)
            alive      = alive.to(self.device)
            know       = know.to(self.device)
            hp         = hp.to(self.device)
            sz         = sz.to(self.device)
            sp         = sp.to(self.device)
            fire       = fire.to(self.device)
            exits      = exits.to(self.device)
            leader_pos = leader_pos.to(self.device)
            leader_alive = leader_alive.to(self.device)

            done = torch.zeros(B, dtype=torch.bool, device=self.device)

            exited_mask = torch.zeros((B, self.num_agents), dtype=torch.bool, device=self.device)
            died_mask   = torch.zeros_like(exited_mask)
            evacuated_mask = torch.zeros_like(alive)
            exit_step   = torch.full_like(exited_mask, -1, dtype=torch.int32)
            hp_at_exit  = torch.zeros_like(exited_mask, dtype=torch.float)

            total_reward = 0.0

            for step in range(1, self.max_steps + 1):
                # ðŸ”¥ ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¾Ð³Ð¾Ð½ÑŒ Ð¸ Ð²Ñ‹Ñ…Ð¾Ð´
                fire, exits = batched_update_fire_exit(fire, exits)

                # Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð² (Ñ†Ð¸ÐºÐ»)
                for env in self.envs:
                    env.step_leaders()

                # ðŸ§± ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñ‹ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¶Ð¸Ð²Ñ‹Ðµ)
                ag, szm, spm, inf, leader_map = batched_update_agents(
                    pos, sz, sp, know, alive, self.grid_size, 
                    leader_pos, leader_alive, self.envs[0].leader_size
                )

                # ðŸ‘ï¸ ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¾Ð±Ð·Ð¾Ñ€Ñ‹
                views = batched_get_views(
                    leader_map, ag, fire, exits, szm, spm, inf, pos, self.view
                )

                # ðŸ§  ÐœÐ°ÑÐºÐ° Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð², Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ñ… Ðº Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÑŽ
                mask = alive & (~know)

                # DEBUG: Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼, ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ ÑƒÑ‡Ð°ÑÑ‚Ð²ÑƒÐµÑ‚ Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸
                #num_active = mask.sum().item()
                #print(f"[step {step:>3}] active agents: {num_active} | views shape: {views.shape}")

                # ðŸŽ® Ð’Ñ‹Ð±Ð¾Ñ€ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ
                actions = self.select_actions(views, mask)

                # ðŸš¶â€â™‚ï¸ Ð¨Ð°Ð³ ÑÑ€ÐµÐ´Ñ‹
                next_pos, rewards, dones, alive, hp, fire, exits, died, exs = \
                    batched_step(pos, actions, sz, sp, fire, exits, hp, leader_pos, leader_alive, self.envs[0].leader_size)

                # ðŸšª Ð—Ð°Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑƒÑˆÐµÐ´ÑˆÐ¸Ñ…
                evacuated_mask |= exs
                alive &= ~evacuated_mask

                # ðŸ“Š ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ
                next_ag, next_szm, next_spm, next_inf, next_leader_map = batched_update_agents(
                    next_pos, sz, sp, know, alive, self.grid_size,
                    leader_pos, leader_alive, self.envs[0].leader_size
                )


                next_views = batched_get_views(
                    next_leader_map, next_ag, fire, exits, next_szm, next_spm, next_inf, next_pos, self.view
                )



                # ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ñ‹ Ð² Ð±ÑƒÑ„ÐµÑ€
                for b in range(B):
                    idxs = torch.nonzero(mask[b], as_tuple=False).squeeze(1)
                    if idxs.numel() == 0:
                        continue

                    s_batch  = views[b, idxs]
                    a_batch  = actions[b, idxs].unsqueeze(1)
                    r_batch  = rewards[b, idxs]
                    ns_batch = next_views[b, idxs]
                    d_batch = dones[b].expand(idxs.shape[0]).float()



                    self.store_transition(s_batch, a_batch, r_batch, ns_batch, d_batch)



                # Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
                total_reward += rewards.sum().item()
                newly_exit = exs & (~exited_mask)
                newly_die  = died  & (~died_mask)
                exited_mask |= newly_exit
                died_mask   |= newly_die
                exit_step[newly_exit]  = step
                hp_at_exit[newly_exit] = hp[newly_exit]

                pos = next_pos
                done |= dones

                l = self.learn_step()
                if l is not None and l > 0:
                    self.loss_sum += l
                    self.loss_cnt += 1

                if done.all():
                    break
                
            total_agents = self.num_agents * self.n_envs  # N Ã— ENV
            evac_cnt = int(exited_mask.sum().item())
            died_cnt = total_agents - evac_cnt
            avg_st   = float(exit_step[exited_mask].float().mean().item()) if evac_cnt else math.nan
            avg_hp   = float(hp_at_exit[exited_mask].mean().item())         if evac_cnt else math.nan
            avg_loss = self.loss_sum / self.loss_cnt if self.loss_cnt else math.nan
            evac_counts = exited_mask.sum(dim=1)  # Ð¿Ð¾ Ð²ÑÐµÐ¼ ENV
            min_evac = int(evac_counts.min().item())
            max_evac = int(evac_counts.max().item())
            try:
                curr_lr = self.opt.param_groups[0]["lr"]
            except Exception:
                curr_lr = float("nan")


            self.metrics.append({
                "episode":   ep + 1,
                "reward":    float(total_reward),
                "evacuated": evac_cnt,
                "died":      died_cnt,
                "avg_steps": avg_st,
                "avg_hp":    avg_hp,
                "epsilon":   float(getattr(self, "epsilon", float("nan"))),
                "loss":      avg_loss,
                "duration":  time.time() - start,
                "lr": curr_lr,
                "evac_min": min_evac,
                "evac_max": max_evac,
            })

            self.loss_sum, self.loss_cnt = 0.0, 0
            self.after_episode(ep)
            
            # ðŸ”½ Automatic learning rate decay
            if hasattr(self, "opt") and self.opt is not None:
                for g in self.opt.param_groups:
                    g["lr"] = max(self.lr_min, g["lr"] * self.lr_decay)

            curr_lr = self.opt.param_groups[0]["lr"] if hasattr(self, "opt") else float("nan")
            if ep == 0:
                print("ALGORITHM:", self.cfg.get("ALGORITHM", "UnknownAlgorithm"))

            print(f"EP {ep + 1}/{self.cfg['NUM_EPISODES']} | time {time.time() - start:>5.2f}s | "
                f"reward {total_reward:>8.1f} | evac {evac_cnt:>3} | "
                f"died {died_cnt:>3} | eps {getattr(self, 'epsilon', float('nan')):>4.3f} | "
                f"lr {curr_lr:.5f} | loss {avg_loss:>6.3f}", flush=True)

        self._save_metrics_csv()
        
        self._plot_metrics()

        for i in range(4):
            if i >= len(self.envs): break 
            self._render_episode(env_idx=i)
            
    def _init_metrics(self):
        self.metrics = []                       
        self.loss_sum, self.loss_cnt = 0.0, 0
        # Ð´Ð»Ñ ÑÐ²Ð°ÐºÑƒÐ°Ñ†Ð¸Ð¸ / ÑÐ¼ÐµÑ€Ñ‚ÐµÐ¹
        env0 = self.envs[0]
        self.num_agents = env0.N              

    def _save_metrics_csv(self):
        fname = self.cfg.get("METRICS_CSV", "metrics.csv")
        df = pd.DataFrame(self.metrics)
        out = pathlib.Path(fname)
        df.to_csv(out, index=False)
        print("[done] metrics saved ->", str(out.resolve().absolute()))


    def _plot_metrics(self):
        df = pd.DataFrame(self.metrics)
        plt.figure(figsize=(12, 12))

        algo = self.cfg.get("ALGORITHM", "Algorithm")
        plt.suptitle(f"{algo} â€” Training Metrics", fontsize=16)

        plt.subplot(4, 2, 1); plt.plot(df["episode"], df["reward"]); plt.title("Reward")

        plt.subplot(4, 2, 2); plt.plot(df["episode"], df["loss"]);    plt.title("Loss")

        plt.subplot(4, 2, 3)
        ax1 = plt.gca()
        ax2 = ax1.twinx()

        # Ð›ÐµÐ²Ð°Ñ Ð¾ÑÑŒ: Ð°Ð±ÑÐ¾Ð»ÑŽÑ‚Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
        ax1.plot(df["episode"], df["evacuated"], label="Evacuated", color="tab:green")
        ax1.plot(df["episode"], df["died"], label="Died", color="tab:red")
        ax1.set_ylabel("Count")
        ax1.legend(loc="upper left")

        # ÐŸÑ€Ð°Ð²Ð°Ñ Ð¾ÑÑŒ: Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚ ÑÐ²Ð°ÐºÑƒÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ…
        total_agents = self.num_agents * self.n_envs
        evac_percent = df["evacuated"] / total_agents * 100
        ax2.plot(df["episode"], evac_percent, label="% Evacuated", color="tab:blue", linestyle="--")
        ax2.set_ylabel("% Evacuated")
        ax2.set_ylim(0, 100)
        ax2.legend(loc="upper right")

        plt.title("Evacuated / Died / % Evacuated")

        plt.subplot(4, 2, 4)
        plt.plot(df["episode"], df["evac_min"], label="min evac")
        plt.plot(df["episode"], df["evac_max"], label="max evac")
        plt.title("Min/Max Evacuated per Env")
        plt.legend()

        plt.subplot(4, 2, 5); plt.plot(df["episode"], df["avg_steps"]); plt.title("Avg steps to exit")
        plt.subplot(4, 2, 6); plt.plot(df["episode"], df["avg_hp"]); plt.title("Avg HP at exit")
        plt.subplot(4, 2, 7); plt.plot(df["episode"], df["epsilon"]); plt.title("Epsilon")
        plt.subplot(4, 2, 8); plt.plot(df["episode"], df["lr"]); plt.title("lr (Learning Rate)")

        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.show()


    # -----------------------------------------------------------------------
    # UNIVERSAL RENDER ------------------------------------------------------
    # -----------------------------------------------------------------------
    def _render_episode(
        self,
        env_idx: int = 0,
        max_steps: int | None = None,
        out_dir: str = "videos",
        scale: int = 10,
    ):
        """ÐŸÑ€Ð¾Ð¸Ð³Ñ€Ð°Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½ ÑÐ¿Ð¸Ð·Ð¾Ð´ Ð±ÐµÐ· Îµ-ÑˆÑƒÐ¼Ð° Ð¸ Ð·Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ MP4-Ñ€Ð¾Ð»Ð¸Ðº.

        â€¢ env_idx   â€“ ÐºÐ°ÐºÑƒÑŽ ÑÑ€ÐµÐ´Ñƒ Ð²Ð·ÑÑ‚ÑŒ (0-based)  
        â€¢ max_steps â€“ Ð¾Ð±Ñ€ÐµÐ·ÐºÐ° Ð¿Ð¾ ÑˆÐ°Ð³Ð°Ð¼ (None â†’ self.max_steps)  
        â€¢ scale     â€“ Ð²Ð¾ ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ€Ð°Ð· Â«ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ Ð¿Ð¸ÐºÑÐµÐ»Ð¸Â» Ð´Ð»Ñ ÐºÑ€Ð°ÑÐ¸Ð²Ð¾Ð¹ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸
        """
        os.makedirs(out_dir, exist_ok=True)
        env       = self.envs[env_idx]
        algo_name = self.__class__.__name__.replace("Trainer", "")
        fname     = os.path.join(
            out_dir,
            f"{algo_name}_env{env_idx+1:02d}.mp4"
        )

        # ---------- Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° ÑÐµÑ‚Ð¸ Ðº Ð´ÐµÑ‚-Ñ€ÐµÐ¶Ð¸Ð¼Ñƒ -------------
        prev_eps          = getattr(self, "epsilon", None)
        self.epsilon      = 0.0                # Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ greedy
        noisy             = hasattr(self.online, "reset_noise")
        self.online.eval()
        self.target.eval()
        if noisy:                                 # Ñ„Ð¸ÐºÑÐ¸Ñ€ÑƒÐµÐ¼ ÑˆÑƒÐ¼ Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð·
            self.online.reset_noise()
            self.target.reset_noise()

        env.reset()
        grid = self.grid_size
        frames = []

        max_steps = max_steps or self.max_steps
        pos, alive, know, hp, sz, sp, fire, exit_m, leader_pos, leader_alive = stack_envs([env])


        for _ in range(max_steps):

            env.step_leaders()

            ag, szm, spm, inf, leader_map = batched_update_agents(
                pos, sz, sp, know, alive, self.grid_size,
                env.leader_positions.unsqueeze(0),
                env.leader_alive.unsqueeze(0),
                env.leader_size
            )
            views = batched_get_views(
                leader_map, ag, fire, exit_m, szm, spm, inf, pos, self.view
            )

            actions = self.select_actions(views, alive & (~know))


            pos, _, done, alive, hp, fire, exit_m, _, _ = batched_step(
                pos, actions, sz, sp, fire, exit_m, hp,
                env.leader_positions.unsqueeze(0),  # [1, L, 2]
                env.leader_alive.unsqueeze(0),      # [1, L]
                env.leader_size
            )

            

            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñƒ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² ÑƒÐ¶Ðµ Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»Ñ‘Ð½Ð½Ñ‹Ð¼ alive
            ag, szm, spm, inf, leader_map = batched_update_agents(
                pos, sz, sp, know, alive, self.grid_size, 
                env.leader_positions.unsqueeze(0),  # [1, num_leaders, 2]
                env.leader_alive.unsqueeze(0),      # [num_leaders]
                env.leader_size
            )

            # --- Ñ€Ð¸ÑÑƒÐµÐ¼ -----------------------------------------------------------------
            rgb = np.zeros((grid, grid, 3), dtype=np.uint8)
            rgb[exit_m[0].cpu().numpy() == 1] = (0, 255, 0)          # Ð²Ñ‹Ñ…Ð¾Ð´ â€“ Ð·ÐµÐ»Ñ‘Ð½Ñ‹Ð¹
            rgb[fire  [0].cpu().numpy() == 1] = (255, 0, 0)          # Ð¾Ð³Ð¾Ð½ÑŒ â€“ ÐºÑ€Ð°ÑÐ½Ñ‹Ð¹
            ys, xs = np.where(ag[0].cpu().numpy() > 0.5)             # Ð°Ð³ÐµÐ½Ñ‚Ñ‹ â€“ ÑÐ¸Ð½Ð¸Ð¹
            rgb[ys, xs] = (0, 0, 255)

            ys, xs = np.where(leader_map[0].cpu().numpy() > 0.5)
            rgb[ys, xs] = (255, 255, 0)  # Ð»Ð¸Ð´ÐµÑ€Ñ‹ â€” Ð¶Ñ‘Ð»Ñ‚Ñ‹Ðµ

            pil = Image.fromarray(rgb).resize(
                (grid * scale, grid * scale), Image.NEAREST
            )
            frames.append(np.asarray(pil))

            if done.item():
                break

            # Ñ€Ð°ÑÐ¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¾Ð³Ð½Ñ â€“ ÐžÐ‘Ð¯Ð—ÐÐ¢Ð•Ð›Ð¬ÐÐž Ð² ÐºÐ¾Ð½Ñ†Ðµ Ñ†Ð¸ÐºÐ»Ð°,
            # Ð¸Ð½Ð°Ñ‡Ðµ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ ÐºÐ°Ð´Ñ€ Ð½Ðµ ÑÐ¾Ð²Ð¿Ð°Ð´Ñ‘Ñ‚ Ñ Â«ÑˆÐ°Ð³Ð¾Ð¼ ÑÑ€ÐµÐ´Ñ‹Â»
            fire, exit_m = batched_update_fire_exit(fire, exit_m)

        # ---------- Ð·Ð°Ð¿Ð¸ÑÑŒ video ----------------------------------------------------------
        with imageio.get_writer(
            fname, fps=5, codec="libx264",
            ffmpeg_params=["-pix_fmt", "yuv420p", "-profile:v", "baseline"]
        ) as vid:
            for f in frames:
                vid.append_data(f)

        # ---------- Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ ---------------------------------------------
        if prev_eps is not None:
            self.epsilon = prev_eps
        print(f"[done] video saved -> {fname!s}")
